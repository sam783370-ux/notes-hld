
throughput -

It's often contrasted with latency. While latency measures the time delay for a single operation to complete, 
throughput measures how many of those operations can be completed in a unit of time.


Throughput is one-way. Latency/Ping/RTT are round-trip.



2. Elaboration & Examples (The "I Can Apply This" Answer)
Be ready to give examples from different domains:

Networking: "For example, in networking, throughput is the actual rate of successful data delivery, measured in bits per second (bps) or Mbps. It's what you effectively get from your internet plan, which can be lower than the theoretical bandwidth due to overhead and congestion."

Databases: "In a database system, throughput is measured in Transactions Per Second (TPS). A higher TPS means the database can handle more user transactions concurrently, which is critical for e-commerce platforms."

CPUs/Storage: "For a CPU, it's Instructions Per Second (IPS), and for a storage device like an SSD, it's Input/Output Operations Per Second (IOPS)."



Cross-Question 1: "What is the difference between throughput and latency?"
Your Answer:
"This is a fundamental distinction.

Latency is the time it takes for a single unit of work to go from start to finish. For example, the time for one packet to travel from my computer to a server and back (round-trip time).

Throughput is the number of such work units that can be processed in a given time window. For example, how many such packets can make the round trip in one second.

Cross-Question 2: "What is the relationship between throughput and latency?"
Your Answer:
"They are deeply related and often involve a trade-off, especially under load.

Under Normal Load: They can be independent.

Under Heavy Load: As you push a system towards its maximum throughput, queues start to build up. This increased queueing time directly increases the latency for each request. So, at high utilization, increasing the offered load (trying to push more throughput) will cause latency to spike non-linearly."

Cross-Question 4: "How would you measure or improve the throughput of a service?"
Your Answer (Showcasing a methodical approach):
"To measure it, I would use a load-testing tool (like Apache JMeter, k6, or wrk) to simulate concurrent users and gradually increase the load until the throughput plateaus or the error rate increases.



Bandwidth is the number of lanes on the highway. A 4-lane highway has a higher capacity (bandwidth) than a 2-lane highway.

Throughput is the actual number of cars passing a point per hour. Even with 4 lanes, a traffic jam (congestion) means your throughput is low.

Latency is the time it takes for one car to travel from point A to point B. A wide, empty highway (high bandwidth) can still be very long (high latency).

So, bandwidth sets the upper limit for how much data can flow, but throughput is what actually flows."